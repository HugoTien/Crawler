{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import concurrent.futures\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient('10.120.27.11', 27017)\n",
    "\n",
    "requests.packages.urllib3.disable_warnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done page4500\n",
      "done page4501\n",
      "done page4502\n",
      "done page4503\n",
      "done page4504\n",
      "done page4505\n",
      "done page4506\n",
      "done page4507\n",
      "done page4508\n",
      "done page4509\n",
      "done page4510\n",
      "done page4511\n",
      "done page4512\n",
      "done page4513\n",
      "done page4514\n",
      "done page4515\n",
      "done page4516\n",
      "done page4517\n",
      "done page4518\n",
      "done page4519\n",
      "done page4520\n",
      "done page4521\n",
      "done page4522\n",
      "done page4523\n",
      "done page4524\n",
      "done page4525\n",
      "done page4526\n",
      "done page4527\n",
      "done page4528\n",
      "done page4529\n",
      "done page4530\n",
      "done page4531\n",
      "done page4532\n",
      "done page4533\n",
      "done page4534\n",
      "done page4535\n",
      "done page4536\n",
      "done page4537\n",
      "done page4538\n",
      "done page4539\n",
      "done page4540\n",
      "done page4541\n",
      "done page4542\n",
      "done page4543\n",
      "done page4544\n",
      "done page4545\n",
      "done page4546\n",
      "done page4547\n",
      "done page4548\n",
      "done page4549\n",
      "done page4550\n",
      "done page4551\n",
      "done page4552\n",
      "done page4553\n",
      "done page4554\n",
      "done page4555\n",
      "done page4556\n",
      "done page4557\n",
      "done page4558\n",
      "done page4559\n",
      "done page4560\n",
      "done page4561\n",
      "done page4562\n",
      "done page4563\n",
      "done page4564\n",
      "done page4565\n",
      "done page4566\n",
      "done page4567\n",
      "done page4568\n",
      "done page4569\n",
      "done page4570\n",
      "done page4571\n",
      "done page4572\n",
      "done page4573\n",
      "done page4574\n",
      "done page4575\n",
      "done page4576\n",
      "done page4577\n",
      "done page4578\n",
      "done page4579\n",
      "done page4580\n",
      "done page4581\n",
      "done page4582\n",
      "done page4583\n",
      "done page4584\n",
      "done page4585\n",
      "done page4586\n",
      "done page4587\n",
      "done page4588\n",
      "done page4589\n",
      "done page4590\n",
      "done page4591\n",
      "done page4592\n",
      "done page4593\n",
      "done page4594\n",
      "done page4595\n",
      "done page4596\n",
      "done page4597\n",
      "done page4598\n",
      "done page4599\n",
      "done page4600\n",
      "done page4601\n",
      "done page4602\n"
     ]
    }
   ],
   "source": [
    "for i in range(4500,5175):\n",
    "    article_title_list = [] # title list\n",
    "    article_link_list = [] # link list\n",
    "    article_author_list = [] # author list\n",
    "    article_monthday_list = [] # month & date lsit\n",
    "    article_year_list = [] # year list\n",
    "    article_detail_list = [] # all mix\n",
    "\n",
    "    author = \"\"\n",
    "    articledate = \"\"\n",
    "    res = requests.get('https://www.ptt.cc/bbs/Japan_Travel/index{}.html'.format(str(i), verify = False))\n",
    "    soup = BeautifulSoup(res.text,\"lxml\")\n",
    "        \n",
    "\n",
    "    bigtitle = soup.find_all('div',class_ ='r-ent')\n",
    "    for every in bigtitle:\n",
    "    \n",
    "        author_list = every.find_all('div','author')  # 作者名稱    \n",
    "        for author in author_list:\n",
    "            author = author.text\n",
    "            article_author_list.append(author)\n",
    "\n",
    "        date_list = every.find_all('div',\"date\") # 文章日期\n",
    "        for monthday in date_list:\n",
    "            monthday = monthday.text.strip().replace(\"/\",\"-\")\n",
    "            article_monthday_list.append(monthday)\n",
    "            \n",
    "\n",
    "\n",
    "        title = every.find_all('div',class_ = 'title') # 標題&網址\n",
    "        for element in title:\n",
    "            try:\n",
    "                article_link = \"https://www.ptt.cc\"+ element.select('a')[0]['href'] # link\n",
    "                article_link_list.append(article_link) # link\n",
    "                #         article_title = element.select('a')\n",
    "                article_title = element.text.replace(\"\\n\",\"\")\n",
    "                article_title_list.append(article_title)\n",
    "    #         except KeyError:\n",
    "    #             article_title = \"\"\n",
    "    #             pass\n",
    "\n",
    "            \n",
    "            except IndexError:\n",
    "                article_link = \"None\"\n",
    "                article_link_list.append(article_link)\n",
    "                article_title = \"None\"\n",
    "                article_title_list.append(article_title)\n",
    "                pass\n",
    "    print(\"done page{}\".format(i))\n",
    "########################################################\n",
    "\n",
    "    for i in range(0,20):\n",
    "        \n",
    "        push_list = []\n",
    "\n",
    "        author = article_author_list[i]\n",
    "        article_monthday = article_monthday_list[i]\n",
    "#         article_year = article_year_list[i]\n",
    "        article_title = article_title_list[i]\n",
    "        article_link = article_link_list[i]\n",
    "\n",
    "        \n",
    "        try:\n",
    "            res = requests.get(str(article_link_list[i]),verify = False)\n",
    "            soup = BeautifulSoup(res.text,\"lxml\")\n",
    "            content = soup.find('div', id='main-container').text\n",
    "                \n",
    "            article_year = soup.find_all('span',class_=\"article-meta-value\")[3].text[-4:]\n",
    "#             article_year_list.append(article_year)\n",
    "\n",
    "\n",
    "            push_all = soup.find_all('div', class_='push')\n",
    "            for push in push_all:\n",
    "                push_id = push.text.split(\":\")[0].split(\" \")[1]\n",
    "                push_type = push.text[0]\n",
    "                push_content = push.text.split(\":\")[1].split(\"/\")[0][:-3]\n",
    "                push_detail = {\n",
    "                            \"id\": push_id,\n",
    "                            \"推噓\": push_type,\n",
    "                            \"回覆內容\": push_content\n",
    "                            }\n",
    "\n",
    "                push_list.append(push_detail)\n",
    "        \n",
    "\n",
    "\n",
    "        except:\n",
    "            content = \"None\"\n",
    "            \n",
    "        article_detail = {\n",
    "            '作者': author,\n",
    "            '日期': article_year + \"-\" + article_monthday,\n",
    "            '標題':article_title,\n",
    "            'url': article_link,\n",
    "            '內文': content,\n",
    "            '回文': push_list\n",
    "        }\n",
    "        client.text_mining.ptt_article.insert_one(article_detail)\n",
    "        article_detail_list.append(article_detail) \n",
    "            \n",
    "    time.sleep(0.1)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    #     article_detail = {\n",
    "    #      'author': author,\n",
    "    #      'article_date': article_date,\n",
    "    #      'article_title':article_title,\n",
    "    #      'article_link': article_link, #\n",
    "    #     #  'content': content\n",
    "    #     }\n",
    "    #     article_detail_list.append(article_detail)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done page5001\n",
      "done page5002\n",
      "done page5003\n",
      "done page5004\n",
      "done page5005\n",
      "done page5006\n",
      "done page5007\n",
      "done page5008\n",
      "done page5009\n",
      "done page5010\n",
      "done page5011\n",
      "done page5012\n",
      "done page5013\n",
      "done page5014\n",
      "done page5015\n",
      "done page5016\n",
      "done page5017\n",
      "done page5018\n",
      "done page5019\n",
      "done page5020\n",
      "done page5021\n",
      "done page5022\n",
      "done page5023\n",
      "done page5024\n",
      "done page5025\n",
      "done page5026\n",
      "done page5027\n",
      "done page5028\n",
      "done page5029\n",
      "done page5030\n",
      "done page5031\n",
      "done page5032\n",
      "done page5033\n",
      "done page5034\n",
      "done page5035\n",
      "done page5036\n",
      "done page5037\n",
      "done page5038\n",
      "done page5039\n",
      "done page5040\n",
      "done page5041\n",
      "done page5042\n",
      "done page5043\n",
      "done page5044\n",
      "done page5045\n",
      "done page5046\n",
      "done page5047\n",
      "done page5048\n",
      "done page5049\n",
      "done page5050\n",
      "done page5051\n",
      "done page5052\n",
      "done page5053\n",
      "done page5054\n",
      "done page5055\n",
      "done page5056\n",
      "done page5057\n",
      "done page5058\n",
      "done page5059\n",
      "done page5060\n",
      "done page5061\n",
      "done page5062\n",
      "done page5063\n",
      "done page5064\n",
      "done page5065\n",
      "done page5066\n",
      "done page5067\n",
      "done page5068\n",
      "done page5069\n",
      "done page5070\n",
      "done page5071\n",
      "done page5072\n",
      "done page5073\n",
      "done page5074\n",
      "done page5075\n",
      "done page5076\n",
      "done page5077\n",
      "done page5078\n",
      "done page5079\n",
      "done page5080\n",
      "done page5081\n",
      "done page5082\n",
      "done page5083\n",
      "done page5084\n",
      "done page5085\n",
      "done page5086\n",
      "done page5087\n",
      "done page5088\n",
      "done page5089\n",
      "done page5090\n",
      "done page5091\n",
      "done page5092\n",
      "done page5093\n",
      "done page5094\n",
      "done page5095\n",
      "done page5096\n",
      "done page5097\n",
      "done page5098\n",
      "done page5099\n",
      "done page5100\n",
      "done page5101\n",
      "done page5102\n",
      "done page5103\n",
      "done page5104\n",
      "done page5105\n",
      "done page5106\n",
      "done page5107\n",
      "done page5108\n",
      "done page5109\n",
      "done page5110\n",
      "done page5111\n",
      "done page5112\n",
      "done page5113\n",
      "done page5114\n",
      "done page5115\n",
      "done page5116\n",
      "done page5117\n",
      "done page5118\n",
      "done page5119\n",
      "done page5120\n",
      "done page5121\n",
      "done page5122\n",
      "done page5123\n",
      "done page5124\n",
      "done page5125\n",
      "done page5126\n",
      "done page5127\n",
      "done page5128\n",
      "done page5129\n",
      "done page5130\n",
      "done page5131\n",
      "done page5132\n",
      "done page5133\n",
      "done page5134\n",
      "done page5135\n",
      "done page5136\n",
      "done page5137\n",
      "done page5138\n",
      "done page5139\n",
      "done page5140\n",
      "done page5141\n",
      "done page5142\n",
      "done page5143\n",
      "done page5144\n",
      "done page5145\n",
      "done page5146\n",
      "done page5147\n",
      "done page5148\n",
      "done page5149\n",
      "done page5150\n",
      "done page5151\n",
      "done page5152\n",
      "done page5153\n",
      "done page5154\n",
      "done page5155\n",
      "done page5156\n",
      "done page5157\n",
      "done page5158\n",
      "done page5159\n",
      "done page5160\n",
      "done page5161\n",
      "done page5162\n",
      "done page5163\n",
      "done page5164\n",
      "done page5165\n",
      "done page5166\n",
      "done page5167\n",
      "done page5168\n",
      "done page5169\n",
      "done page5170\n",
      "done page5171\n",
      "done page5172\n",
      "done page5173\n",
      "done page5174\n"
     ]
    }
   ],
   "source": [
    "article_title_list = [] # title list\n",
    "article_link_list = [] # link list\n",
    "article_author_list = [] # author list\n",
    "article_date_list = [] # date list\n",
    "article_detail_list4 = [] # all mix\n",
    "author = \"\"\n",
    "articledate = \"\"\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:\n",
    "    for i in range(5001,5175):\n",
    "        res = requests.get('https://www.ptt.cc/bbs/Japan_Travel/index{}.html'.format(str(i), verify = False))\n",
    "        soup = BeautifulSoup(res.text,\"lxml\")\n",
    "\n",
    "\n",
    "        bigtitle = soup.find_all('div',class_ ='r-ent')\n",
    "        for every in bigtitle:\n",
    "\n",
    "            author_list = every.find_all('div','author')  # 作者名稱    \n",
    "            for author in author_list:\n",
    "                author = author.text\n",
    "                article_author_list.append(author)\n",
    "\n",
    "            date_list = every.find_all('div',\"date\") # 文章日期\n",
    "            for article_date in date_list:\n",
    "                article_date = article_date.text.strip()\n",
    "                article_date_list.append(article_date)\n",
    "\n",
    "\n",
    "            title = every.find_all('div',class_ = 'title') # 標題&網址\n",
    "            for element in title:\n",
    "                try:\n",
    "                    article_link = \"https://www.ptt.cc\"+ element.select('a')[0]['href'] # link\n",
    "                    article_link_list.append(article_link) # link\n",
    "                    #         article_title = element.select('a')\n",
    "                    article_title = element.text.replace(\"\\n\",\"\")\n",
    "                    article_title_list.append(article_title)\n",
    "        #         except KeyError:\n",
    "        #             article_title = \"\"\n",
    "        #             pass\n",
    "                except IndexError:\n",
    "                    article_link = \"None\"\n",
    "                    article_link_list.append(article_link)\n",
    "                    article_title = \"None\"\n",
    "                    article_title_list.append(article_title)\n",
    "                    pass\n",
    "        print(\"done page{}\".format(i))\n",
    "    ########################################################\n",
    "\n",
    "        for i in range(0,20):\n",
    "\n",
    "            author = article_author_list[i]\n",
    "            article_date = article_date_list[i]\n",
    "            article_title = article_title_list[i]\n",
    "            article_link = article_link_list[i]\n",
    "\n",
    "            try:\n",
    "                res = requests.get(str(article_link_list[i]),verify = False)\n",
    "                soup = BeautifulSoup(res.text,\"lxml\")\n",
    "                content = soup.find('div', id='main-container').text\n",
    "            except:\n",
    "                content = \"None\"\n",
    "\n",
    "            article_detail = {\n",
    "                'author': author,\n",
    "                'article_date': article_date,\n",
    "                'article_title':article_title,\n",
    "                'article_link': article_link,\n",
    "                'content': content\n",
    "            }\n",
    "            client.text_mining.ptt_article.insert_one(article_detail)\n",
    "            article_detail_list4.append(article_detail)   \n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #     article_detail = {\n",
    "        #      'author': author,\n",
    "        #      'article_date': article_date,\n",
    "        #      'article_title':article_title,\n",
    "        #      'article_link': article_link, #\n",
    "        #     #  'content': content\n",
    "        #     }\n",
    "        #     article_detail_list.append(article_detail)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'article_title_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8f4599f430a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0marticle_title_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'article_title_list' is not defined"
     ]
    }
   ],
   "source": [
    "article_title_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article_title_list = [] # title list\n",
    "article_link_list = [] # link list\n",
    "article_author_list = [] # author list\n",
    "article_date_list = [] # date list\n",
    "article_detail_list = [] # all mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'article_detail_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-bfd2c8a63996>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_detail_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'article_detail_list' is not defined"
     ]
    }
   ],
   "source": [
    "len(article_detail_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Java\\Anaconda3\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py:852: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "res = requests.get('https://www.ptt.cc/bbs/Japan_Travel/M.1224405182.A.37B.html',verify = False)\n",
    "soup = BeautifulSoup(res.text,\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE html>\n",
       "<html>\n",
       "<head>\n",
       "<meta charset=\"utf-8\"/>\n",
       "<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
       "<title>[遊記] 激安～超便宜生猛海鮮哪裡找？！函館ダイニング雅家篇 - 看板 Japan_Travel - 批踢踢實業坊</title>\n",
       "<meta content=\"all\" name=\"robots\"/>\n",
       "<meta content=\"Ptt BBS 批踢踢\" name=\"keywords\"/>\n",
       "<meta content=\"精采圖文版請見http://blog.xuite.net/yukochan/200408/20108834  \n",
       "函館朝市有非常多的海產店，當然一定少不了螃蟹，可是好貴喔～～一隻便宜點的螃蟹幾\n",
       "乎都要4,5千圓，更貴的當然更多，YUKO這天不住旅館，根本沒法處理一整隻的螃蟹呀（\n",
       "泣……），所以想看看還有沒有單賣蟹腳的，看了半天也沒沒看到單賣蟹腳的，其他的生\n",
       "魚片、海膽、烏賊等各式海鮮也都不便宜，轉頭看看網路上大家推薦的海鮮食堂、餐廳，\n",
       "\" name=\"description\"/>\n",
       "<meta content=\"Ptt 批踢踢實業坊\" property=\"og:site_name\"/>\n",
       "<meta content=\"[遊記] 激安～超便宜生猛海鮮哪裡找？！函館ダイニング雅家篇\" property=\"og:title\"/>\n",
       "<meta content=\"精采圖文版請見http://blog.xuite.net/yukochan/200408/20108834  \n",
       "函館朝市有非常多的海產店，當然一定少不了螃蟹，可是好貴喔～～一隻便宜點的螃蟹幾\n",
       "乎都要4,5千圓，更貴的當然更多，YUKO這天不住旅館，根本沒法處理一整隻的螃蟹呀（\n",
       "泣……），所以想看看還有沒有單賣蟹腳的，看了半天也沒沒看到單賣蟹腳的，其他的生\n",
       "魚片、海膽、烏賊等各式海鮮也都不便宜，轉頭看看網路上大家推薦的海鮮食堂、餐廳，\n",
       "\" property=\"og:description\"/>\n",
       "<link href=\"https://www.ptt.cc/bbs/Japan_Travel/M.1224405182.A.37B.html\" rel=\"canonical\"/>\n",
       "<link href=\"//images.ptt.cc/bbs/v2.22/bbs-common.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
       "<link href=\"//images.ptt.cc/bbs/v2.22/bbs-base.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n",
       "<link href=\"//images.ptt.cc/bbs/v2.22/bbs-custom.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
       "<link href=\"//images.ptt.cc/bbs/v2.22/pushstream.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n",
       "<link href=\"//images.ptt.cc/bbs/v2.22/bbs-print.css\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/>\n",
       "</head>\n",
       "<body>\n",
       "<div id=\"fb-root\"></div>\n",
       "<script>(function(d, s, id) {\n",
       "var js, fjs = d.getElementsByTagName(s)[0];\n",
       "if (d.getElementById(id)) return;\n",
       "js = d.createElement(s); js.id = id;\n",
       "js.src = \"//connect.facebook.net/en_US/all.js#xfbml=1\";\n",
       "fjs.parentNode.insertBefore(js, fjs);\n",
       "}(document, 'script', 'facebook-jssdk'));</script>\n",
       "<div id=\"topbar-container\">\n",
       "<div class=\"bbs-content\" id=\"topbar\">\n",
       "<a href=\"/\" id=\"logo\">批踢踢實業坊</a>\n",
       "<span>›</span>\n",
       "<a class=\"board\" href=\"/bbs/Japan_Travel/index.html\"><span class=\"board-label\">看板 </span>Japan_Travel</a>\n",
       "<a class=\"right small\" href=\"/about.html\">關於我們</a>\n",
       "<a class=\"right small\" href=\"/contact.html\">聯絡資訊</a>\n",
       "</div>\n",
       "</div>\n",
       "<div id=\"navigation-container\">\n",
       "<div class=\"bbs-content\" id=\"navigation\">\n",
       "<a class=\"board\" href=\"/bbs/Japan_Travel/index.html\">返回看板</a>\n",
       "<div class=\"bar\"></div>\n",
       "<div class=\"share\">\n",
       "<span>分享</span>\n",
       "<div class=\"fb-like\" data-href=\"http://www.ptt.cc/bbs/Japan_Travel/M.1224405182.A.37B.html\" data-layout=\"button_count\" data-send=\"false\" data-show-faces=\"false\" data-width=\"90\"></div>\n",
       "<div class=\"g-plusone\" data-size=\"medium\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "window.___gcfg = {lang: 'zh-TW'};\n",
       "(function() {\n",
       "var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;\n",
       "po.src = 'https://apis.google.com/js/plusone.js';\n",
       "var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);\n",
       "})();\n",
       "</script>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "<div id=\"main-container\">\n",
       "<div class=\"bbs-screen bbs-content\" id=\"main-content\"><div class=\"article-metaline\"><span class=\"article-meta-tag\">作者</span><span class=\"article-meta-value\">gyoko (YUKO)</span></div><div class=\"article-metaline-right\"><span class=\"article-meta-tag\">看板</span><span class=\"article-meta-value\">Japan_Travel</span></div><div class=\"article-metaline\"><span class=\"article-meta-tag\">標題</span><span class=\"article-meta-value\">[遊記] 激安～超便宜生猛海鮮哪裡找？！函館ダイニング雅家篇</span></div><div class=\"article-metaline\"><span class=\"article-meta-tag\">時間</span><span class=\"article-meta-value\">Sun Oct 19 16:32:59 2008</span></div>\n",
       "<span class=\"f3 hl\">精采圖文版請見<a href=\"http://blog.xuite.net/yukochan/200408/20108834\" rel=\"nofollow\" target=\"_blank\">http://blog.xuite.net/yukochan/200408/20108834</a> </span>\n",
       "\n",
       "函館朝市有非常多的海產店，當然一定少不了螃蟹，可是好貴喔～～一隻便宜點的螃蟹幾\n",
       "乎都要4,5千圓，更貴的當然更多，YUKO這天不住旅館，根本沒法處理一整隻的螃蟹呀（\n",
       "泣……），所以想看看還有沒有單賣蟹腳的，看了半天也沒沒看到單賣蟹腳的，其他的生\n",
       "魚片、海膽、烏賊等各式海鮮也都不便宜，轉頭看看網路上大家推薦的海鮮食堂、餐廳，\n",
       "大多店家的櫥窗都設有食物模型的玻璃櫥窗，嗯～～雖然看起來很好吃，但好貴喔，大多\n",
       "比YUKO昨晚在小樽吃到的海鮮丼小碗且貴好多喔，才半個多小時YUKO就把函館朝市逛完了\n",
       "<span class=\"f6\">&gt; &lt;\n",
       "</span>\n",
       "正當YUKO在考慮是不是要放棄而進観光案內所拿DM時，忽然發現就在函館朝市正對角有一\n",
       "間水產批發商（函館朝市「北榮」<a href=\"http://www.asaiti.com\" rel=\"nofollow\" target=\"_blank\">http://www.asaiti.com</a>）經營的海鮮餐廳「函館ダイ\n",
       "ニング雅家」，午餐有限定御膳共11道菜才1260圓，而且憑DM的折價券還可兌換主廚推薦\n",
       "一品！YUKO當場就決定「就是你啦！！！」，毫不猶豫的踏進店內。\n",
       "\n",
       "\n",
       "<span class=\"f6 hl\">函館ダイニング雅家 <a href=\"http://www.asaiti.com/pc/html/insyoku/gaya/gaya.html\" rel=\"nofollow\" target=\"_blank\">http://www.asaiti.com/pc/html/insyoku/gaya/gaya.html</a> </span>\n",
       "\n",
       "YUKO是非假日的中午11點左右店剛開就來了，因為只有一個人，所以被安排在一樓的吧台\n",
       "區，坐在這因為可以親眼目睹大廚們一展手藝，未預約的客人一般而言只能坐樓上，YUKO\n",
       "坐的位置是在吧台最邊邊，其他位置都已放上預約牌了，在YUKO後面進來的客人全都被請\n",
       "到2、3樓了，看來YUKO這次又很幸運的坐到個好位（笑～～）。\n",
       "\n",
       "\n",
       "<span class=\"f6 hl\">函館ダイニング雅家有推出「限定ランチ」（限定午餐）三種\n",
       "</span><span class=\"f6 hl\"><a href=\"http://www.asaiti.com/pc/html/insyoku/gaya/gaya-lanch.html\" rel=\"nofollow\" target=\"_blank\">http://www.asaiti.com/pc/html/insyoku/gaya/gaya-lanch.html</a> </span>\n",
       "<span class=\"b1 hl\">●お刺身膳(全11品)・・・1,260円</span>\n",
       "旬の刺身3点盛り、蟹と季節の野菜の天麩羅、煮物、茶碗蒸し、サラダ、小鉢、小付け\n",
       "、ご飯、蟹汁、香の物、デザート\n",
       "<span class=\"b1 hl\">●焼き魚膳(全11品)・・・1,260円</span>\n",
       "焼魚3点盛り、蟹と季節の野菜の天麩羅、煮物、茶碗蒸し、サラダ、小鉢、小付け、ご\n",
       "飯、蟹汁、香の物、デザート\n",
       "<span class=\"b1 hl\">●蕎麦膳(全9品)・・・840円</span>\n",
       "蕎麦、蟹と季節の野菜の天麩羅、煮物、茶碗蒸し、サラダ、小鉢、小付け、香の物、デ\n",
       "ザート\n",
       "\n",
       "\n",
       "YUKO點的是第一種「お刺身膳」（生魚片御膳），一端上來真的是嚇到YUKO了，真的好豐\n",
       "富喔！！！！（這樣才1260圓耶，真的是太便宜了！！）\n",
       "\n",
       "\n",
       "蟹腳、草蝦和當季蔬菜現炸的天麩羅，非常的香脆，而且明確襯托出每道食才的特色，蟹\n",
       "腳和草蝦肉質非常Q、非常有彈性，完美呈現當季蔬菜的甜味，吃了完全不會有油膩感。\n",
       "生魚片的部分是鮭魚、鰹魚、章魚和干貝，都很新鮮甘甜，肉質也還保有彈性，很不錯。\n",
       "\n",
       "\n",
       "茶碗蒸超棒的！！他們家的茶碗蒸應該有加海鮮高湯下去做，所以不僅吃得到蛋的香醇還\n",
       "吃得到海鮮的鮮甜味，最棒的是整個茶碗蒸的口感跟烤布蕾一樣，非常的綿密細緻，但不\n",
       "會讓人覺得單薄，反而有一種濃厚感，好吃到YUKO想問他們可不可以單點追加茶碗蒸（笑\n",
       "～～沒辦法YUKO最喜歡的東西就是海鮮跟蛋料理了，根本沒有抵抗力XD）。\n",
       "\n",
       "\n",
       "味增湯一樣是蟹腳味增湯，肉滿多的！一旁的豆腐是手工豆腐，豆味很濃純，口感緊實，\n",
       "很好吃。飯的部分是用北海道「道南産ふっくりん」，非常Q非常好吃，而且最棒的是午\n",
       "餐時飯可無限量取用喔！！！其他小菜YUKO就不特別介紹了～\n",
       "\n",
       "\n",
       "但YUKO要特別介紹的是他們有附上一罐<span class=\"f6 hl\">「無着色たらこ」（無添加色素純天然的鹽漬鱈魚\n",
       "</span><span class=\"f6 hl\">子，就是照片中位於生魚片右上角那罐看起來向蕃茄醬的東西）</span>！！一般餐廳附上鹽漬鱈\n",
       "魚子、明太子之類時，都是只有一小碟或是放一小匙在飯上，但雅家可是直接大方的給一\n",
       "罐喔！！！這是他們家水產店自己產銷的產品，據說賣的很好呢，這也是只有午餐才提供\n",
       "的，而且是無限量提供喔！！這鹽漬鱈魚子單吃很鹹（畢竟是鹽漬的），但放在飯上超好\n",
       "吃、超下飯的！！但還有一個東西加這鹽漬鱈魚子也超好吃的，就是加在用DM折價券所附\n",
       "贈的特製沙拉（其實YUKO搞不大清楚那白色條狀物是什麼，本來以為是白蘿蔔絲，但吃了\n",
       "也不是，可是坐在YUKO身邊的日本女生都非常羨慕，後來都紛紛加點了，好像還不便宜耶\n",
       "！有沒有人知道這是什麼呀？？？真的好好奇喔～拜託知道的人告訴YUKO吧^^）。這特製\n",
       "沙拉本身有附醬汁，但後來YUKO發現拌上鹽漬鱈魚子後會變得更好吃耶！！\n",
       "\n",
       "\n",
       "這餐YUKO真的是撐到吃不下，而且YUKO的餐送上來時剛好有好幾組客人進來，每一組客人\n",
       "看到YUKO的餐後就好奇的問店員YUKO點的是哪道，尤其在得知這是午餐限定才1260圓後每\n",
       "個客人都要點這道，但因為數量有限，所以YUKO要離開前今天的「お刺身膳」已經全數供\n",
       "應完畢了，後面晚到的客人個個都一臉惋惜呀！！！如果大家到函館午餐想吃有別於海鮮\n",
       "丼的料理的話，真的非常推薦雅家的午餐限定喔^0^\n",
       "\n",
       "\n",
       "--\n",
       "日本關西吃喝玩樂趴趴走\n",
       "<a href=\"http://blog.xuite.net/yukochan/200408\" rel=\"nofollow\" target=\"_blank\">http://blog.xuite.net/yukochan/200408</a>\n",
       "\n",
       "--\n",
       "<span class=\"f2\">※ 發信站: 批踢踢實業坊(ptt.cc) \n",
       "</span>◆ From: 124.8.17.60\n",
       "<div class=\"push\"><span class=\"hl push-tag\">推 </span><span class=\"f3 hl push-userid\">sophiacandy</span><span class=\"f3 push-content\">:好好吃的樣子</span><span class=\"push-ipdatetime\"> 10/20 15:54\n",
       "</span></div><div class=\"push\"><span class=\"hl push-tag\">推 </span><span class=\"f3 hl push-userid\">uzumaki</span><span class=\"f3 push-content\">:好想吃喔~~</span><span class=\"push-ipdatetime\"> 10/20 16:24\n",
       "</span></div></div>\n",
       "<div data-longpollurl=\"/v1/longpoll?id=2c99322a98401417ecc9c719db9402d050318646\" data-offset=\"4304\" data-pollurl=\"/poll/Japan_Travel/M.1224405182.A.37B.html?cacheKey=2052-68551254&amp;offset=4304&amp;offset-sig=366f07d3c28a96268d81350f9a16f79e3ac6d32f\" id=\"article-polling\"></div>\n",
       "</div>\n",
       "<script>\n",
       "  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n",
       "  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n",
       "  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n",
       "  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n",
       "\n",
       "  ga('create', 'UA-32365737-1', {\n",
       "    cookieDomain: 'ptt.cc',\n",
       "    legacyCookieDomain: 'ptt.cc'\n",
       "  });\n",
       "  ga('send', 'pageview');\n",
       "</script>\n",
       "<script src=\"//ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js\"></script>\n",
       "<script src=\"//images.ptt.cc/bbs/v2.22/bbs.js\"></script>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = soup.find('div', id='main-container')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n作者gyoko (YUKO)看板Japan_Travel標題[遊記] 激安～超便宜生猛海鮮哪裡找？！函館ダイニング雅家篇時間Sun Oct 19 16:32:59 2008\\n精采圖文版請見http://blog.xuite.net/yukochan/200408/20108834 \\n\\n函館朝市有非常多的海產店，當然一定少不了螃蟹，可是好貴喔～～一隻便宜點的螃蟹幾\\n乎都要4,5千圓，更貴的當然更多，YUKO這天不住旅館，根本沒法處理一整隻的螃蟹呀（\\n泣……），所以想看看還有沒有單賣蟹腳的，看了半天也沒沒看到單賣蟹腳的，其他的生\\n魚片、海膽、烏賊等各式海鮮也都不便宜，轉頭看看網路上大家推薦的海鮮食堂、餐廳，\\n大多店家的櫥窗都設有食物模型的玻璃櫥窗，嗯～～雖然看起來很好吃，但好貴喔，大多\\n比YUKO昨晚在小樽吃到的海鮮丼小碗且貴好多喔，才半個多小時YUKO就把函館朝市逛完了\\n> <\\n\\n正當YUKO在考慮是不是要放棄而進観光案內所拿DM時，忽然發現就在函館朝市正對角有一\\n間水產批發商（函館朝市「北榮」http://www.asaiti.com）經營的海鮮餐廳「函館ダイ\\nニング雅家」，午餐有限定御膳共11道菜才1260圓，而且憑DM的折價券還可兌換主廚推薦\\n一品！YUKO當場就決定「就是你啦！！！」，毫不猶豫的踏進店內。\\n\\n\\n函館ダイニング雅家 http://www.asaiti.com/pc/html/insyoku/gaya/gaya.html \\n\\nYUKO是非假日的中午11點左右店剛開就來了，因為只有一個人，所以被安排在一樓的吧台\\n區，坐在這因為可以親眼目睹大廚們一展手藝，未預約的客人一般而言只能坐樓上，YUKO\\n坐的位置是在吧台最邊邊，其他位置都已放上預約牌了，在YUKO後面進來的客人全都被請\\n到2、3樓了，看來YUKO這次又很幸運的坐到個好位（笑～～）。\\n\\n\\n函館ダイニング雅家有推出「限定ランチ」（限定午餐）三種\\nhttp://www.asaiti.com/pc/html/insyoku/gaya/gaya-lanch.html \\n●お刺身膳(全11品)・・・1,260円\\n旬の刺身3点盛り、蟹と季節の野菜の天麩羅、煮物、茶碗蒸し、サラダ、小鉢、小付け\\n、ご飯、蟹汁、香の物、デザート\\n●焼き魚膳(全11品)・・・1,260円\\n焼魚3点盛り、蟹と季節の野菜の天麩羅、煮物、茶碗蒸し、サラダ、小鉢、小付け、ご\\n飯、蟹汁、香の物、デザート\\n●蕎麦膳(全9品)・・・840円\\n蕎麦、蟹と季節の野菜の天麩羅、煮物、茶碗蒸し、サラダ、小鉢、小付け、香の物、デ\\nザート\\n\\n\\nYUKO點的是第一種「お刺身膳」（生魚片御膳），一端上來真的是嚇到YUKO了，真的好豐\\n富喔！！！！（這樣才1260圓耶，真的是太便宜了！！）\\n\\n\\n蟹腳、草蝦和當季蔬菜現炸的天麩羅，非常的香脆，而且明確襯托出每道食才的特色，蟹\\n腳和草蝦肉質非常Q、非常有彈性，完美呈現當季蔬菜的甜味，吃了完全不會有油膩感。\\n生魚片的部分是鮭魚、鰹魚、章魚和干貝，都很新鮮甘甜，肉質也還保有彈性，很不錯。\\n\\n\\n茶碗蒸超棒的！！他們家的茶碗蒸應該有加海鮮高湯下去做，所以不僅吃得到蛋的香醇還\\n吃得到海鮮的鮮甜味，最棒的是整個茶碗蒸的口感跟烤布蕾一樣，非常的綿密細緻，但不\\n會讓人覺得單薄，反而有一種濃厚感，好吃到YUKO想問他們可不可以單點追加茶碗蒸（笑\\n～～沒辦法YUKO最喜歡的東西就是海鮮跟蛋料理了，根本沒有抵抗力XD）。\\n\\n\\n味增湯一樣是蟹腳味增湯，肉滿多的！一旁的豆腐是手工豆腐，豆味很濃純，口感緊實，\\n很好吃。飯的部分是用北海道「道南産ふっくりん」，非常Q非常好吃，而且最棒的是午\\n餐時飯可無限量取用喔！！！其他小菜YUKO就不特別介紹了～\\n\\n\\n但YUKO要特別介紹的是他們有附上一罐「無着色たらこ」（無添加色素純天然的鹽漬鱈魚\\n子，就是照片中位於生魚片右上角那罐看起來向蕃茄醬的東西）！！一般餐廳附上鹽漬鱈\\n魚子、明太子之類時，都是只有一小碟或是放一小匙在飯上，但雅家可是直接大方的給一\\n罐喔！！！這是他們家水產店自己產銷的產品，據說賣的很好呢，這也是只有午餐才提供\\n的，而且是無限量提供喔！！這鹽漬鱈魚子單吃很鹹（畢竟是鹽漬的），但放在飯上超好\\n吃、超下飯的！！但還有一個東西加這鹽漬鱈魚子也超好吃的，就是加在用DM折價券所附\\n贈的特製沙拉（其實YUKO搞不大清楚那白色條狀物是什麼，本來以為是白蘿蔔絲，但吃了\\n也不是，可是坐在YUKO身邊的日本女生都非常羨慕，後來都紛紛加點了，好像還不便宜耶\\n！有沒有人知道這是什麼呀？？？真的好好奇喔～拜託知道的人告訴YUKO吧^^）。這特製\\n沙拉本身有附醬汁，但後來YUKO發現拌上鹽漬鱈魚子後會變得更好吃耶！！\\n\\n\\n這餐YUKO真的是撐到吃不下，而且YUKO的餐送上來時剛好有好幾組客人進來，每一組客人\\n看到YUKO的餐後就好奇的問店員YUKO點的是哪道，尤其在得知這是午餐限定才1260圓後每\\n個客人都要點這道，但因為數量有限，所以YUKO要離開前今天的「お刺身膳」已經全數供\\n應完畢了，後面晚到的客人個個都一臉惋惜呀！！！如果大家到函館午餐想吃有別於海鮮\\n丼的料理的話，真的非常推薦雅家的午餐限定喔^0^\\n\\n\\n--\\n日本關西吃喝玩樂趴趴走\\nhttp://blog.xuite.net/yukochan/200408\\n\\n--\\n※ 發信站: 批踢踢實業坊(ptt.cc) \\n◆ From: 124.8.17.60\\n推 sophiacandy:好好吃的樣子 10/20 15:54\\n推 uzumaki:好想吃喔~~ 10/20 16:24\\n\\n\\n'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
